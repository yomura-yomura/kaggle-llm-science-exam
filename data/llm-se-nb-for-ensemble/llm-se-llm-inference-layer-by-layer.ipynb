{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LLM + Wikipedia RAG\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# # Quick Save\n",
    "# import os\n",
    "# if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     ! touch submission.csv\n",
    "#     import sys\n",
    "#     sys.exit(0)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T14:57:36.656526Z",
     "iopub.execute_input": "2023-10-03T14:57:36.657415Z",
     "iopub.status.idle": "2023-10-03T14:57:36.663002Z",
     "shell.execute_reply.started": "2023-10-03T14:57:36.657378Z",
     "shell.execute_reply": "2023-10-03T14:57:36.662106Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install --quiet --use-deprecated=legacy-resolver --no-index /kaggle/input/llm-se-python-wheel/llm_science_exam-0.0.1-py3-none-any.whl --find-links /kaggle/input/llm-se-required-libs-python-wheels"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-10-03T14:57:36.669076Z",
     "iopub.execute_input": "2023-10-03T14:57:36.669469Z",
     "iopub.status.idle": "2023-10-03T14:57:51.405790Z",
     "shell.execute_reply.started": "2023-10-03T14:57:36.669446Z",
     "shell.execute_reply": "2023-10-03T14:57:51.404654Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Create symlinks from kaggle datasets to fake cached model\n",
    "# import pathlib\n",
    "# checkpoint_path = pathlib.Path(\"/root/.cache/\")\n",
    "# checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# for part in [1, 2]:\n",
    "#     source_dir = pathlib.Path(f'/kaggle/input/platypus2-70b-instruct-part{part}')\n",
    "#     for path in source_dir.glob('*'):\n",
    "#         try:\n",
    "#             (checkpoint_path / path.name).symlink_to(path)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "# checkpoint_path = \"/kaggle/input/llmse-llama2-13b-layers-es-with-full-td\"\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T14:57:51.407913Z",
     "iopub.execute_input": "2023-10-03T14:57:51.408275Z",
     "iopub.status.idle": "2023-10-03T14:57:51.413197Z",
     "shell.execute_reply.started": "2023-10-03T14:57:51.408243Z",
     "shell.execute_reply": "2023-10-03T14:57:51.412082Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gc\n",
    "from time import time, sleep\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ctypes\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# For Platypus2\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils.modeling import set_module_tensor_to_device\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T14:57:51.414561Z",
     "iopub.execute_input": "2023-10-03T14:57:51.415225Z",
     "iopub.status.idle": "2023-10-03T14:58:04.352615Z",
     "shell.execute_reply.started": "2023-10-03T14:57:51.415192Z",
     "shell.execute_reply": "2023-10-03T14:58:04.351695Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "\n",
    "import os\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    dataset_type = \"test\"\n",
    "else:\n",
    "    dataset_type = \"train\"\n",
    "#     \n",
    "# df = pd.read_csv(f\"/kaggle/input/kaggle-llm-science-exam/{dataset_type}.csv\", index_col=\"id\")\n",
    "# # df = df.iloc[:2]\n",
    "# \n",
    "# # # +300 validation\n",
    "# # if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "# #     extra_df = pd.read_csv(\"/kaggle/input/llm-se-extra-train-datasets/yalickj/dataset-wiki-new-1/dataset_wiki_new_1_balanced.csv\")\n",
    "# #     df = pd.concat([df, extra_df]).reset_index(drop=True)\n",
    "# #     df.index.name = \"id\"\n",
    "# \n",
    "# # if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "# #    df = pd.concat([df] * 8).reset_index(drop=True)\n",
    "# #    df.index.name = \"id\"\n",
    "# \n",
    "# df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T14:58:04.354958Z",
     "iopub.execute_input": "2023-10-03T14:58:04.355518Z",
     "iopub.status.idle": "2023-10-03T14:58:04.400803Z",
     "shell.execute_reply.started": "2023-10-03T14:58:04.355486Z",
     "shell.execute_reply": "2023-10-03T14:58:04.399737Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               prompt  \\\nid                                                      \n0   Which of the following statements accurately d...   \n1   Which of the following is an accurate definiti...   \n\n                                                    A  \\\nid                                                      \n0   MOND is a theory that reduces the observed mis...   \n1   Dynamic scaling refers to the evolution of sel...   \n\n                                                    B  \\\nid                                                      \n0   MOND is a theory that increases the discrepanc...   \n1   Dynamic scaling refers to the non-evolution of...   \n\n                                                    C  \\\nid                                                      \n0   MOND is a theory that explains the missing bar...   \n1   Dynamic scaling refers to the evolution of sel...   \n\n                                                    D  \\\nid                                                      \n0   MOND is a theory that reduces the discrepancy ...   \n1   Dynamic scaling refers to the non-evolution of...   \n\n                                                    E answer  \nid                                                            \n0   MOND is a theory that eliminates the observed ...      D  \n1   Dynamic scaling refers to the evolution of sel...      A  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>answer</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>MOND is a theory that reduces the observed mis...</td>\n      <td>MOND is a theory that increases the discrepanc...</td>\n      <td>MOND is a theory that explains the missing bar...</td>\n      <td>MOND is a theory that reduces the discrepancy ...</td>\n      <td>MOND is a theory that eliminates the observed ...</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which of the following is an accurate definiti...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>Dynamic scaling refers to the non-evolution of...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>Dynamic scaling refers to the non-evolution of...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>A</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Wikipedia Retrieval Augmented Generation (RAG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code is adapted from https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# !cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/\n",
    "# \n",
    "# import llm_science_exam.open_book_v2\n",
    "# \n",
    "# df[\"context\"] = llm_science_exam.open_book_v2.tf_idf.get_context(\n",
    "#     df,\n",
    "#     wiki_dataset_paths=[\n",
    "#         \"/kaggle/working/all-paraphs-parsed-expanded\",\n",
    "# #         \"/kaggle/input/llm-se-additional-wiki-stem-articles\"\n",
    "#     ],\n",
    "#     num_titles=3,\n",
    "# #     num_titles=4,\n",
    "# )\n",
    "# clean_memory()\n",
    "# \n",
    "# print(df.iloc[0])\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T14:58:16.887482Z",
     "iopub.execute_input": "2023-10-03T14:58:16.889277Z",
     "iopub.status.idle": "2023-10-03T15:05:32.562471Z",
     "shell.execute_reply.started": "2023-10-03T14:58:16.889240Z",
     "shell.execute_reply": "2023-10-03T15:05:32.561626Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/2101279 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1ad3d2d12634096a92aebce3862f23b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "length of vectorizer vocab is 224\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 211/211 [04:16<00:00,  1.21s/it]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "prompt     Which of the following statements accurately d...\nA          MOND is a theory that reduces the observed mis...\nB          MOND is a theory that increases the discrepanc...\nC          MOND is a theory that explains the missing bar...\nD          MOND is a theory that reduces the discrepancy ...\nE          MOND is a theory that eliminates the observed ...\nanswer                                                     D\ncontext    - MOND is an example of a class of theories kn...\nName: 0, dtype: object\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_with_context_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "system_prefix = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be pertinent.\n",
    "\n",
    "### Input:\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{prompt}\n",
    "\n",
    "Proposed answer:\n",
    "\"\"\"\n",
    "\n",
    "def get_prompts(row):\n",
    "    prompt_prefix = system_prefix.format(context=row[\"context\"], prompt=row[\"prompt\"])\n",
    "\n",
    "    prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in 'ABCDE']\n",
    "\n",
    "    return prompt_prefix, prompt_suffix\n",
    "\n",
    "\n",
    "prefix, suffixes = get_prompts(df.iloc[0])\n",
    "print(f\"{prefix}{suffixes[0]}\")\n",
    "del prefix, suffixes"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:05:32.566398Z",
     "iopub.execute_input": "2023-10-03T15:05:32.568326Z",
     "iopub.status.idle": "2023-10-03T15:05:32.580089Z",
     "shell.execute_reply.started": "2023-10-03T15:05:32.568294Z",
     "shell.execute_reply": "2023-10-03T15:05:32.579391Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYour task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be pertinent.\n\n### Input:\nContext:\n- MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable. Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called \"RMOND\", for \"relativistic MOND\", which had \"been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum.\" \n- Milgrom's law fully specifies the rotation curve of a galaxy given only the distribution of its baryonic mass. In particular, MOND predicts a far stronger correlation between features in the baryonic mass distribution and features in the rotation curve than does the dark matter hypothesis (since dark matter dominates the galaxy's mass budget and is conventionally assumed not to closely track the distribution of baryons). Such a tight correlation is claimed to be observed in several spiral galaxies, a fact which has been referred to as \"Renzo's rule\".\n- At a statistical significance of 8σ, it was found that the spatial offset of the center of the total mass from the center of the baryonic mass peaks cannot be explained with an alteration of the gravitational force law.\n\nQuestion:\nWhich of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed \"missing baryonic mass\" discrepancy in galaxy clusters?\n\nProposed answer:\nMOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called \"fuzzy dark matter.\"\n\n### Response:\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tokens(row, tokenizer, max_length):\n",
    "    prompt_prefix, prompt_suffix = get_prompts(row)\n",
    "    \n",
    "    prefix = tokenizer(\n",
    "        prompt_prefix, return_tensors=\"pt\", return_attention_mask=False, \n",
    "        truncation=True, max_length=max_length,\n",
    "    )['input_ids']\n",
    "    \n",
    "    suffix = tokenizer(\n",
    "        prompt_suffix, return_tensors=\"pt\", return_attention_mask=False,\n",
    "        truncation=True, max_length=max_length, \n",
    "        padding=True\n",
    "    )['input_ids'][:, 1:]\n",
    "    \n",
    "    return prefix, suffix\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "inputs = df.apply(lambda row: get_tokens(row, tokenizer, max_length=None), axis=1).values\n",
    "MAX_LENGTH = max(p.shape[1] + s.shape[1] for p, s in inputs)\n",
    "\n",
    "del inputs, tokenizer\n",
    "clean_memory()\n",
    "\n",
    "MAX_LENGTH = min(MAX_LENGTH, 4096)\n",
    "\n",
    "print(f\"{MAX_LENGTH = }\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:05:32.584249Z",
     "iopub.execute_input": "2023-10-03T15:05:32.586405Z",
     "iopub.status.idle": "2023-10-03T15:05:33.070636Z",
     "shell.execute_reply.started": "2023-10-03T15:05:32.586374Z",
     "shell.execute_reply": "2023-10-03T15:05:33.069671Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
     "output_type": "stream"
    },
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "751"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2: Run\n",
    "\n",
    "To such a large model on a single T4 GPU, we run it layer by layer and sample by sample"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Class for sharded llama\n",
    "\n",
    "\n",
    "class ShardedLlama:\n",
    "    \n",
    "    def __init__(self, checkpoint_path, device: int, dtype=torch.float16):\n",
    "        \"\"\"\n",
    "        Sharded version of LlamaForCausalLM : the model is splitted into layer shards to reduce GPU memory usage.\n",
    "        During the forward pass, the inputs are processed layer by layer, and the GPU memory is freed after each layer.\n",
    "        To avoid loading the layers multiple times, we could save all the intermediate activations in RAM, but\n",
    "        as Kaggle accelerators have more GPU memory than CPU, we simply batch the inputs and keep them on the GPU.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        checkpoint_path : str or Path\n",
    "            path to the checkpoint\n",
    "        device : device\n",
    "        dtype : torch.dtype, optional\n",
    "            dtype, by default torch.float16\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save parameters\n",
    "        self.checkpoint_path = Path(checkpoint_path)\n",
    "        self.device = f\"cuda:{device}\"\n",
    "        self.device_id = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Create model\n",
    "        self.config = AutoConfig.from_pretrained(self.checkpoint_path)\n",
    "        # For flash attention when Turing architecture will be supported : https://github.com/Dao-AILab/flash-attention/issues/542\n",
    "        # self.config.auto_map = {\"AutoModelForCausalLM\" : \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"} \n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = 'right'\n",
    "        \n",
    "        self.tokenizer_pad_token_id = self.tokenizer.pad_token_id\n",
    "        \n",
    "        self.init_model()        \n",
    "        self.layer_names = ['model.embed_tokens'] + [f'model.layers.{i}' for i in range(len(self.model.model.layers))] + ['model.norm', 'lm_head']\n",
    "        \n",
    "    def init_model(self):\n",
    "        \n",
    "        # Load meta model (no memory used)\n",
    "        with init_empty_weights():\n",
    "            self.model = AutoModelForCausalLM.from_config(self.config, trust_remote_code=True)\n",
    "            self.model.tie_weights()\n",
    "        \n",
    "        self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n",
    "        \n",
    "        # Move buffers to device (not that much GPU memory used)\n",
    "        for buffer_name, buffer in self.model.named_buffers():\n",
    "            set_module_tensor_to_device(self.model, buffer_name, self.device, value=buffer, dtype=self.dtype)\n",
    "\n",
    "    def load_layer(self, layer_name):\n",
    "        state_dict = load_file(self.checkpoint_path / (layer_name + '.safetensors'), device=self.device)\n",
    "        for param_name, param in state_dict.items():\n",
    "            assert param.dtype != torch.int8, 'int8 not supported (need to add fp16_statistics)'\n",
    "            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, inputs, output_token: int | list[int]):\n",
    "        # inputs = [(prefix, suffix), ...] with prefix.shape[0] = 1 and suffix.shape[0] = 5\n",
    "        \n",
    "        # Reboot the model to make sure buffers are loaded and memory is clean\n",
    "        del self.model\n",
    "        clean_memory()\n",
    "        self.init_model()\n",
    "        \n",
    "        # Send batch to device\n",
    "        batch = [(prefix.to(self.device), suffix.to(self.device)) for prefix, suffix in inputs]\n",
    "        n_suffixes = len(batch[0][1])\n",
    "        suffix_eos = [(suffix != self.tokenizer_pad_token_id).sum(1) - 1 for _, suffix in inputs]\n",
    "\n",
    "        # Create attention mask for the largest input, and position ids to use KV cache\n",
    "        attention_mask = torch.finfo(self.dtype).min * torch.ones(MAX_LENGTH, MAX_LENGTH)\n",
    "        attention_mask = attention_mask.triu(diagonal=1)[None, None, ...]\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        position_ids = torch.arange(MAX_LENGTH, dtype=torch.long, device=self.device)[None, :]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor, torch.inference_mode():\n",
    "\n",
    "            # Load first layer\n",
    "            future = executor.submit(self.load_layer, 'model.embed_tokens')\n",
    "\n",
    "            for i, (layer_name, layer) in enumerate(zip(tqdm(self.layer_names, desc=f\"inference layer by layer on device {self.device}\", position=self.device_id+1), self.layers)):\n",
    "\n",
    "                # Wait for previous layer to be loaded and load next layer\n",
    "                start = time()\n",
    "                future.result()\n",
    "                if (i + 1) < len(self.layer_names):\n",
    "                    future = executor.submit(self.load_layer, self.layer_names[i + 1])\n",
    "                load_time = time() - start\n",
    "                \n",
    "                # Run layer\n",
    "                for j, (prefix, suffix) in enumerate(batch):\n",
    "                    if layer_name == 'model.embed_tokens':\n",
    "                        batch[j] = (layer(prefix), layer(suffix))\n",
    "                    elif layer_name == 'model.norm':\n",
    "                        # Only keep the last hidden state at this point\n",
    "                        batch[j] = (None, layer(suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None]))\n",
    "                    elif layer_name == 'lm_head':\n",
    "                        batch[j] = (None, layer(suffix))\n",
    "                    else:\n",
    "                        # Run prefix\n",
    "                        len_p, len_s = prefix.shape[1], suffix.shape[1]\n",
    "                        new_prefix, (k_cache, v_cache) = layer(prefix, use_cache=True, attention_mask=attention_mask[:, :, -len_p:, -len_p:])\n",
    "                        \n",
    "                        # Run suffix\n",
    "#                         pos = position_ids[:, len_p:len_p + len_s].repeat(n_suffixes, 1)\n",
    "#                         attn = attention_mask[:, :, -len_s:, -len_p - len_s:].repeat(n_suffixes, 1, 1, 1)\n",
    "#                         kv_cache = (k_cache.repeat(n_suffixes, 1, 1, 1), v_cache.repeat(n_suffixes, 1, 1, 1))\n",
    "                        pos = position_ids[:, len_p:len_p + len_s].expand(n_suffixes, -1)\n",
    "                        attn = attention_mask[:, :, -len_s:, -len_p - len_s:].expand(n_suffixes, -1, -1, -1)\n",
    "                        kv_cache = (k_cache.repeat(n_suffixes, 1, 1, 1), v_cache.expand(n_suffixes, -1, -1, -1))\n",
    "\n",
    "                        new_suffix = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=attn)[0]\n",
    "                        batch[j] = (new_prefix, new_suffix)\n",
    "                \n",
    "                # Remove previous layer from memory (including buffers)\n",
    "                layer.to('meta')\n",
    "                clean_memory() # proposed by CPMP\n",
    "        \n",
    "        # Get scores\n",
    "        if isinstance(output_token, list):\n",
    "            batch = [torch.softmax(suffix[:, -1, output_token], dim=1)[..., 0].detach().cpu().numpy() for _, suffix in batch]\n",
    "        else:\n",
    "            batch = [suffix[:, -1, output_token].detach().cpu().numpy() for _, suffix in batch]\n",
    "\n",
    "        return batch\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:05:33.072169Z",
     "iopub.execute_input": "2023-10-03T15:05:33.072833Z",
     "iopub.status.idle": "2023-10-03T15:05:33.092174Z",
     "shell.execute_reply.started": "2023-10-03T15:05:33.072799Z",
     "shell.execute_reply": "2023-10-03T15:05:33.091131Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run model on the 2 GPUs\n",
    "# N_BATCHES = 4\n",
    "# N_BATCHES = 8\n",
    "N_BATCHES = -1\n",
    "\n",
    "# N_BATCHES = max(int(np.ceil(MAX_LENGTH / 1024)), N_BATCHES)\n",
    "N_BATCHES = max(int(np.ceil(MAX_LENGTH / 512)), N_BATCHES)\n",
    "print(f\"{N_BATCHES = }\")\n",
    "\n",
    "\n",
    "def run_model(device: int, df):\n",
    "    sleep(60 * device)\n",
    "    clean_memory()\n",
    "        \n",
    "    model = ShardedLlama(checkpoint_path, device=device, dtype=torch.float16)\n",
    "    inputs = df.apply(partial(get_tokens, tokenizer=model.tokenizer, max_length=MAX_LENGTH), axis=1).values\n",
    "    \n",
    "    del model.tokenizer\n",
    "    clean_memory()\n",
    "    \n",
    "    batches = np.array_split(inputs, N_BATCHES)\n",
    "    outputs = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"* batch #{i + 1} of {len(batches)} on device {device}\")\n",
    "#         outputs += model(batch, output_token=4874)\n",
    "        outputs += model(batch, output_token=[4874, 694])\n",
    "#         outputs += model(batch, output_token=[3582, 1217])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Run model\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    logits = list(executor.map(run_model, [0, 1], np.array_split(df, 2)))\n",
    "    logits = sum(logits, [])\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:05:33.095533Z",
     "iopub.execute_input": "2023-10-03T15:05:33.095782Z",
     "iopub.status.idle": "2023-10-03T15:14:18.269948Z",
     "shell.execute_reply.started": "2023-10-03T15:05:33.095760Z",
     "shell.execute_reply": "2023-10-03T15:14:18.268916Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "N_BATCHES = 1\n* batch #1 of 1 on device 1\n* batch #1 of 1 on device 0\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "inference layer by layer on device cuda:1:   0%|          | 0/43 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89f98632ff734eb0b7ef44e29999bb58"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "inference layer by layer on device cuda:0:   0%|          | 0/43 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a78fde14580143be8a1b4bd3191296b8"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "clean_memory()\n",
    "preds = torch.softmax(torch.Tensor(logits), dim=1).numpy()\n",
    "pd.DataFrame(preds).to_csv(\"probs.csv\", index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:14:18.281772Z",
     "iopub.execute_input": "2023-10-03T15:14:18.282076Z",
     "iopub.status.idle": "2023-10-03T15:14:18.569889Z",
     "shell.execute_reply.started": "2023-10-03T15:14:18.282051Z",
     "shell.execute_reply": "2023-10-03T15:14:18.568916Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_28/571884911.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  preds = torch.softmax(torch.Tensor(logits), dim=1).numpy()\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save results\n",
    "n = len(df)\n",
    "for i, scores in enumerate(preds):\n",
    "    top3 = np.argsort(scores)[::-1]\n",
    "    df.loc[i, 'prediction'] = ' '.join(['ABCDE'[j] for j in top3])\n",
    "df[['prediction']].to_csv('submission.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:14:18.571324Z",
     "iopub.execute_input": "2023-10-03T15:14:18.571922Z",
     "iopub.status.idle": "2023-10-03T15:14:18.619961Z",
     "shell.execute_reply.started": "2023-10-03T15:14:18.571886Z",
     "shell.execute_reply": "2023-10-03T15:14:18.618992Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def print_map_at_3(df):\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        df.loc[i, 'top_1'] = df.loc[i, 'prediction'][0]\n",
    "        df.loc[i, 'top_2'] = df.loc[i, 'prediction'][2]\n",
    "        df.loc[i, 'top_3'] = df.loc[i, 'prediction'][4]\n",
    "\n",
    "    top_i = [(df[f'top_{i}'] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n",
    "    print(f'top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})')\n",
    "    print(f'Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%')\n",
    "\n",
    "    \n",
    "if 'answer' in df.columns:\n",
    "    if len(df) > 200:\n",
    "        print(\"Old CV:\")\n",
    "        print_map_at_3(df.iloc[:200])\n",
    "\n",
    "        print(\"\\nNew CV:\")\n",
    "    else:\n",
    "        print(\"CV:\")\n",
    "    print_map_at_3(df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-03T15:14:18.622991Z",
     "iopub.execute_input": "2023-10-03T15:14:18.623273Z",
     "iopub.status.idle": "2023-10-03T15:14:18.637164Z",
     "shell.execute_reply.started": "2023-10-03T15:14:18.623253Z",
     "shell.execute_reply": "2023-10-03T15:14:18.636257Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "top1 : 2/2, top2 : 0/2, top3 : 0/2 (total=2 / 2)\nAccuracy: 100.0%, map3: 100.0%\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
