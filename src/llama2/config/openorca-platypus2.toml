project_name = "01-base"
exp_name = "with_extra_+60k"

[model]
family = "OpenOrca-Platypus2"
size = "13B"

[dataset]
prompt_id = 1
additional_datasets = [
    # "radek1/additional-train-data-for-llm-science-exam",
    # "radek1/15k-high-quality-examples",
    # "leonidkulyk/wikipedia-stem-1k",
    "cdeotte/60k-data-with-context-v2",
]
train_test_split = false
test_size = 1

[train]
per_device_train_batch_size = 4
per_device_eval_batch_size = 8
gradient_accumulation_steps = 2

learning_rate = 5e-5
weight_decay = 0.001

logging_steps = 200
save_steps = 200
save_total_limit = 3

lr_scheduler_type = "constant"
#lr_scheduler_type = "cosine"
warmup_steps = 1000

num_train_epochs = 5

early_stopping_patience=5
