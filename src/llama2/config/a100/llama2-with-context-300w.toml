#project_name = "02-trained-with-only-extra-datasets"
project_name = "05-with-context"

exp_name = "300w-4"

[model]
family = "Llama2"
size = "7B"
#size = "13B"

[dataset]
prompt_id = 6

additional_datasets = [
    # "radek1/additional-train-data-for-llm-science-exam",
    # "radek1/15k-high-quality-examples",
    # "leonidkulyk/wikipedia-stem-1k",
    "cdeotte/60k-data-with-context-v2",
]
train_test_split = false
test_size = 1
with_context = true

[dataset.context]
upper_limit_of_n_words = 300

[train]
per_device_train_batch_size = 8
per_device_eval_batch_size = 16
gradient_accumulation_steps = 2

learning_rate = 16e-5
weight_decay = 0.001

logging_steps = 200
save_steps = 200
save_total_limit = 2

#lr_scheduler_type = "constant"
lr_scheduler_type = "cosine"
warmup_steps = 500

num_train_epochs = 10

early_stopping_patience=5
