#project_name = "02-trained-with-only-extra-datasets"
project_name = "06-32bits"

exp_name = "base"

[model]
family = "Llama2"
#size = "7B"
size = "13B"
quant_n_bits = 32

[dataset]
prompt_id = 1

additional_datasets = [
    # "radek1/additional-train-data-for-llm-science-exam",
    # "radek1/15k-high-quality-examples",
    # "leonidkulyk/wikipedia-stem-1k",
    "cdeotte/60k-data-with-context-v2",
]
train_test_split = false
test_size = 1
with_context = false

[train]
#per_device_train_batch_size = 8
#per_device_eval_batch_size = 16
#gradient_accumulation_steps = 2
#
#learning_rate = 16e-5
#
#logging_steps = 200
#save_steps = 200

per_device_train_batch_size = 1
per_device_eval_batch_size = 1
gradient_accumulation_steps = 2

logging_steps = 400
save_steps = 400


learning_rate = 2e-5

weight_decay = 0.001

save_total_limit = 2

#lr_scheduler_type = "constant"
lr_scheduler_type = "cosine"
warmup_steps = 500

num_train_epochs = 2

early_stopping_patience=10

[train.lora]
r = 8
alpha = 64
dropout = 0.1
