#project_name = "02-trained-with-only-extra-datasets"
project_name = "06-16bits"

exp_name = "yes_or_no_as_answer_with_context_v2"
#exp_name = "yes_or_no_as_answer_with_context_from_ckpt"

[model]
family = "Llama2"
#size = "7B"
size = "13B"
quant_n_bits = 16

[dataset]
prompt_id = 9

additional_datasets = [
#     "radek1/additional-train-data-for-llm-science-exam",
#     "radek1/15k-high-quality-examples",
#     "leonidkulyk/wikipedia-stem-1k",
    "cdeotte/60k-data-with-context-v2",
#    "cdeotte/40k-data-with-context-v2/ScienceQA"
]
train_test_split = false
test_size = 1
with_context = true

[dataset.context]
#upper_limit_of_n_words = 200
#upper_limit_of_n_words = 300
upper_limit_of_n_words = -1

top_n_sentences = 5

[train]
#per_device_train_batch_size = 8
#per_device_eval_batch_size = 16
#gradient_accumulation_steps = 2
#
#learning_rate = 16e-5
#
#logging_steps = 200
#save_steps = 200

per_device_train_batch_size = 1
per_device_eval_batch_size = 1
gradient_accumulation_steps = 2

logging_steps = 800
save_steps = 800
#logging_steps = 400
#save_steps = 400
#logging_steps = 10
#save_steps = 10

learning_rate = 2e-5

weight_decay = 0.001

save_total_limit = 2

#lr_scheduler_type = "constant"
lr_scheduler_type = "cosine"
warmup_steps = 100

num_train_epochs = 2
#num_train_epochs = 10

early_stopping_patience=10

[train.lora]
#r = 32
r = 16
#r = 8
alpha = 64
dropout = 0.1
